{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# importing libs\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import Input, Dense, GaussianNoise,Lambda,Dropout\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras import regularizers\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adam,SGD\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the first stage training of the two-stage training autoencoder for the relay network\n",
    "# S-R link traing, to determine the source encoding and relay decoding\n",
    "# for n = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reproducing reslut\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M: 256 k: 8 n: 8 R: 1.0\n"
     ]
    }
   ],
   "source": [
    "# defining parameters\n",
    "# define (n,k) here for (n,k) autoencoder\n",
    "# n = n_channel \n",
    "# k = log2(M)  ==> so for (7,4) autoencoder n_channel = 7 and M = 2^4 = 16 \n",
    "M = 2**8\n",
    "k = np.log2(M)\n",
    "k = int(k)\n",
    "n_channel = 8\n",
    "R = k/n_channel\n",
    "print ('M:',M,'k:',k,'n:',n_channel,'R:',R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating data of size N\n",
    "N = 8000\n",
    "label = np.random.randint(M,size=N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating one hot encoded vectors\n",
    "data = []\n",
    "for i in label:\n",
    "    temp = np.zeros(M)\n",
    "    temp[i] = 1\n",
    "    data.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 256)\n"
     ]
    }
   ],
   "source": [
    "# checking data shape\n",
    "data = np.array(data)\n",
    "print (data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "212 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "61 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "3 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "153 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "23 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "76 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "47 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "188 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# checking generated data with it's label\n",
    "temp_check = [17,23,45,67,89,96,72,250,350]\n",
    "for i in temp_check:\n",
    "    print(label[i],data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frange(x, y, jump):\n",
    "  while x < y:\n",
    "    yield x\n",
    "    x += jump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/anaconda3/envs/tf_env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/tf_env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/tf_env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/tf_env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/tf_env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4115: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# defining autoencoder and it's layer\n",
    "input_signal = Input(shape=(M,))\n",
    "encoded = Dense(M, activation='tanh')(input_signal) \n",
    "encoded1 = Dense(n_channel, activation='linear')(encoded)\n",
    "\n",
    "def mynormalization(x):\n",
    "  from keras import backend as K\n",
    "  return np.sqrt(n_channel)*K.l2_normalize(x,axis=1) \n",
    "# Energy per channel use will be 1 (unit power), fair comparison with PSK\n",
    "\n",
    "encoded2 = Lambda(mynormalization)(encoded1)\n",
    "# per-channel noise variance, same as per-bit for R=1\n",
    "encoded3 = GaussianNoise(np.sqrt(1/(2*R*10.0**(np.random.randint(-2, 8.5) /10.0))))(encoded2)\n",
    "# relay detection\n",
    "decoded = Dense(M, activation='linear')(encoded3) # \n",
    "decoded1 = Dense(M, activation='softmax')(decoded) # \n",
    "\n",
    "autoencoder_SR = Model(input_signal, decoded1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/anaconda3/envs/tf_env/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/tf_env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "adam = Adam(lr=0.001)\n",
    "autoencoder_SR.compile(optimizer=adam, loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 2056      \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "gaussian_noise_1 (GaussianNo (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               2304      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               65792     \n",
      "=================================================================\n",
      "Total params: 135,944\n",
      "Trainable params: 135,944\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# printing summary of layers and it's trainable parameters \n",
    "print (autoencoder_SR.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for tensor board visualization\n",
    "#tbCallBack = keras.callbacks.TensorBoard(log_dir='./logs', histogram_freq=0, batch_size=32, write_graph=True, write_grads=True, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/anaconda3/envs/tf_env/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/tf_env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/tf_env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/tf_env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Epoch 1/80\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/tf_env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/tf_env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/tf_env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/tf_env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/tf_env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "8000/8000 [==============================] - 5s 656us/step - loss: 4.1804\n",
      "Epoch 2/80\n",
      "8000/8000 [==============================] - 2s 304us/step - loss: 2.7336\n",
      "Epoch 3/80\n",
      "8000/8000 [==============================] - 3s 328us/step - loss: 2.3053\n",
      "Epoch 4/80\n",
      "8000/8000 [==============================] - 3s 387us/step - loss: 2.1666\n",
      "Epoch 5/80\n",
      "8000/8000 [==============================] - 3s 326us/step - loss: 2.1372\n",
      "Epoch 6/80\n",
      "8000/8000 [==============================] - 2s 312us/step - loss: 2.0525\n",
      "Epoch 7/80\n",
      "8000/8000 [==============================] - 3s 336us/step - loss: 2.0828\n",
      "Epoch 8/80\n",
      "8000/8000 [==============================] - 3s 324us/step - loss: 2.0363\n",
      "Epoch 9/80\n",
      "8000/8000 [==============================] - 2s 301us/step - loss: 2.0576\n",
      "Epoch 10/80\n",
      "8000/8000 [==============================] - 3s 331us/step - loss: 2.0429\n",
      "Epoch 11/80\n",
      "8000/8000 [==============================] - 3s 321us/step - loss: 2.0230\n",
      "Epoch 12/80\n",
      "8000/8000 [==============================] - 3s 336us/step - loss: 2.0243\n",
      "Epoch 13/80\n",
      "8000/8000 [==============================] - 3s 324us/step - loss: 2.0137\n",
      "Epoch 14/80\n",
      "8000/8000 [==============================] - 2s 310us/step - loss: 2.0025\n",
      "Epoch 15/80\n",
      "8000/8000 [==============================] - 3s 313us/step - loss: 1.9932\n",
      "Epoch 16/80\n",
      "8000/8000 [==============================] - 3s 396us/step - loss: 2.0211\n",
      "Epoch 17/80\n",
      "8000/8000 [==============================] - 3s 320us/step - loss: 1.9944\n",
      "Epoch 18/80\n",
      "8000/8000 [==============================] - 3s 323us/step - loss: 1.9781\n",
      "Epoch 19/80\n",
      "8000/8000 [==============================] - 3s 346us/step - loss: 2.0470\n",
      "Epoch 20/80\n",
      "8000/8000 [==============================] - 3s 327us/step - loss: 2.0033\n",
      "Epoch 21/80\n",
      "8000/8000 [==============================] - 3s 329us/step - loss: 1.9921\n",
      "Epoch 22/80\n",
      "8000/8000 [==============================] - 3s 317us/step - loss: 2.0171\n",
      "Epoch 23/80\n",
      "8000/8000 [==============================] - 3s 364us/step - loss: 1.9682\n",
      "Epoch 24/80\n",
      "8000/8000 [==============================] - 3s 342us/step - loss: 1.9849\n",
      "Epoch 25/80\n",
      "8000/8000 [==============================] - 2s 304us/step - loss: 1.9936\n",
      "Epoch 26/80\n",
      "8000/8000 [==============================] - 2s 306us/step - loss: 2.0019\n",
      "Epoch 27/80\n",
      "8000/8000 [==============================] - 3s 339us/step - loss: 1.9932\n",
      "Epoch 28/80\n",
      "8000/8000 [==============================] - 3s 379us/step - loss: 1.9834\n",
      "Epoch 29/80\n",
      "8000/8000 [==============================] - 3s 323us/step - loss: 1.9737\n",
      "Epoch 30/80\n",
      "8000/8000 [==============================] - 3s 333us/step - loss: 1.9935\n",
      "Epoch 31/80\n",
      "8000/8000 [==============================] - 3s 323us/step - loss: 1.9886\n",
      "Epoch 32/80\n",
      "8000/8000 [==============================] - 3s 334us/step - loss: 2.0023\n",
      "Epoch 33/80\n",
      "8000/8000 [==============================] - 3s 319us/step - loss: 1.9884\n",
      "Epoch 34/80\n",
      "8000/8000 [==============================] - 3s 368us/step - loss: 1.9682\n",
      "Epoch 35/80\n",
      "8000/8000 [==============================] - 3s 325us/step - loss: 1.9578\n",
      "Epoch 36/80\n",
      "8000/8000 [==============================] - 3s 351us/step - loss: 2.0517\n",
      "Epoch 37/80\n",
      "8000/8000 [==============================] - 3s 326us/step - loss: 1.9995\n",
      "Epoch 38/80\n",
      "8000/8000 [==============================] - 3s 347us/step - loss: 1.9874\n",
      "Epoch 39/80\n",
      "8000/8000 [==============================] - 3s 348us/step - loss: 2.0037\n",
      "Epoch 40/80\n",
      "8000/8000 [==============================] - 3s 333us/step - loss: 1.9998\n",
      "Epoch 41/80\n",
      "8000/8000 [==============================] - 3s 355us/step - loss: 1.9843\n",
      "Epoch 42/80\n",
      "8000/8000 [==============================] - 3s 321us/step - loss: 1.9738\n",
      "Epoch 43/80\n",
      "8000/8000 [==============================] - 2s 311us/step - loss: 1.9683\n",
      "Epoch 44/80\n",
      "8000/8000 [==============================] - 3s 321us/step - loss: 1.9897\n",
      "Epoch 45/80\n",
      "8000/8000 [==============================] - 3s 344us/step - loss: 1.9592\n",
      "Epoch 46/80\n",
      "8000/8000 [==============================] - 3s 318us/step - loss: 1.9537\n",
      "Epoch 47/80\n",
      "8000/8000 [==============================] - 3s 365us/step - loss: 2.0003\n",
      "Epoch 48/80\n",
      "8000/8000 [==============================] - 3s 347us/step - loss: 1.9325\n",
      "Epoch 49/80\n",
      "8000/8000 [==============================] - 3s 336us/step - loss: 1.9681\n",
      "Epoch 50/80\n",
      "8000/8000 [==============================] - 3s 363us/step - loss: 1.9807\n",
      "Epoch 51/80\n",
      "8000/8000 [==============================] - 3s 408us/step - loss: 1.9585\n",
      "Epoch 52/80\n",
      "8000/8000 [==============================] - 3s 406us/step - loss: 1.9766\n",
      "Epoch 53/80\n",
      "8000/8000 [==============================] - 3s 371us/step - loss: 1.9801\n",
      "Epoch 54/80\n",
      "8000/8000 [==============================] - 3s 366us/step - loss: 1.9648\n",
      "Epoch 55/80\n",
      "8000/8000 [==============================] - 3s 319us/step - loss: 1.9480\n",
      "Epoch 56/80\n",
      "8000/8000 [==============================] - 3s 336us/step - loss: 1.9685\n",
      "Epoch 57/80\n",
      "8000/8000 [==============================] - 3s 353us/step - loss: 1.9685\n",
      "Epoch 58/80\n",
      "8000/8000 [==============================] - 3s 358us/step - loss: 1.9789\n",
      "Epoch 59/80\n",
      "8000/8000 [==============================] - 3s 338us/step - loss: 1.9709\n",
      "Epoch 60/80\n",
      "8000/8000 [==============================] - 3s 381us/step - loss: 1.9452\n",
      "Epoch 61/80\n",
      "8000/8000 [==============================] - 3s 347us/step - loss: 1.9594\n",
      "Epoch 62/80\n",
      "8000/8000 [==============================] - 3s 355us/step - loss: 1.9239\n",
      "Epoch 63/80\n",
      "8000/8000 [==============================] - 3s 356us/step - loss: 1.9605\n",
      "Epoch 64/80\n",
      "8000/8000 [==============================] - 3s 376us/step - loss: 1.9312\n",
      "Epoch 65/80\n",
      "8000/8000 [==============================] - 3s 376us/step - loss: 1.9597\n",
      "Epoch 66/80\n",
      "8000/8000 [==============================] - 3s 353us/step - loss: 1.9487\n",
      "Epoch 67/80\n",
      "8000/8000 [==============================] - 3s 325us/step - loss: 1.9536\n",
      "Epoch 68/80\n",
      "8000/8000 [==============================] - 3s 346us/step - loss: 1.9528\n",
      "Epoch 69/80\n",
      "8000/8000 [==============================] - 3s 325us/step - loss: 1.9098\n",
      "Epoch 70/80\n",
      "8000/8000 [==============================] - 3s 330us/step - loss: 1.9857\n",
      "Epoch 71/80\n",
      "8000/8000 [==============================] - 3s 321us/step - loss: 1.9915\n",
      "Epoch 72/80\n",
      "8000/8000 [==============================] - 3s 325us/step - loss: 1.9530\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/80\n",
      "8000/8000 [==============================] - 3s 351us/step - loss: 1.9310\n",
      "Epoch 74/80\n",
      "8000/8000 [==============================] - 3s 319us/step - loss: 1.9449\n",
      "Epoch 75/80\n",
      "8000/8000 [==============================] - 2s 302us/step - loss: 1.9362\n",
      "Epoch 76/80\n",
      "8000/8000 [==============================] - 3s 328us/step - loss: 1.9818\n",
      "Epoch 77/80\n",
      "8000/8000 [==============================] - 2s 302us/step - loss: 1.9683\n",
      "Epoch 78/80\n",
      "8000/8000 [==============================] - 2s 312us/step - loss: 1.9470\n",
      "Epoch 79/80\n",
      "8000/8000 [==============================] - 2s 305us/step - loss: 1.9772\n",
      "Epoch 80/80\n",
      "8000/8000 [==============================] - 3s 326us/step - loss: 1.9651\n"
     ]
    }
   ],
   "source": [
    "# traning auto encoder\n",
    "hist=autoencoder_SR.fit(data, data,\n",
    "                epochs=80,\n",
    "                batch_size=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_1\n",
      "[]\n",
      "dense_1\n",
      "[array([[-0.03485795, -0.09312075, -0.01856875, ...,  0.0477182 ,\n",
      "         0.07682346,  0.10623781],\n",
      "       [-0.03797137,  0.11708888, -0.10711638, ..., -0.03618057,\n",
      "         0.3061855 , -0.13295652],\n",
      "       [ 0.04183083,  0.07549759, -0.2165226 , ...,  0.10170979,\n",
      "         0.23636386, -0.10616776],\n",
      "       ...,\n",
      "       [-0.17220442,  0.13694054, -0.17908585, ...,  0.15658356,\n",
      "         0.31790265,  0.03979917],\n",
      "       [-0.20103617,  0.03800648,  0.13624527, ..., -0.19704346,\n",
      "         0.06989356, -0.0893888 ],\n",
      "       [ 0.02975768, -0.08755005, -0.00066905, ..., -0.03568891,\n",
      "        -0.19141604,  0.09947579]], dtype=float32), array([ 1.2506972e-04, -6.6676536e-03,  6.7121414e-03, -1.6147555e-03,\n",
      "        4.3270686e-03, -2.6672166e-03,  1.9608501e-03,  3.4822901e-03,\n",
      "       -2.3381942e-04,  5.9381016e-03, -4.4254164e-04, -1.1828599e-03,\n",
      "       -6.4034276e-03, -4.2287363e-03,  5.3620618e-03, -9.6915208e-04,\n",
      "        3.0080631e-04, -6.0701845e-03, -8.9952105e-04,  1.0770720e-03,\n",
      "        4.4778599e-03, -1.3109276e-03,  6.2467922e-03,  3.9100708e-03,\n",
      "       -3.5134717e-03, -2.4687580e-04,  3.5760168e-05,  6.2136952e-04,\n",
      "       -9.0623746e-04, -1.1174507e-03, -1.5715228e-03,  9.2095142e-05,\n",
      "        2.8742366e-03,  1.0665895e-03, -8.4943697e-04,  5.1610610e-03,\n",
      "       -5.1226211e-03,  1.3622149e-03, -5.5461982e-03,  6.3332673e-03,\n",
      "        3.4448977e-03, -3.7756823e-03,  9.3724846e-04, -1.2375059e-03,\n",
      "       -5.7269796e-03,  3.6086785e-03, -3.8776863e-03,  1.0767514e-03,\n",
      "       -1.5351460e-03, -2.7419003e-03, -3.0292815e-03,  7.5912378e-03,\n",
      "        1.1188427e-03, -9.7458865e-03,  3.2147815e-04,  8.3104806e-04,\n",
      "       -9.1701718e-03,  2.1490755e-03, -3.1898657e-03, -6.5565296e-03,\n",
      "       -1.7972712e-03,  2.5461370e-03,  6.5170960e-03, -5.9818500e-03,\n",
      "       -1.2403575e-03,  1.1260009e-03, -8.8050552e-03, -1.8252579e-03,\n",
      "       -1.7706982e-03,  8.3976742e-03, -1.9124795e-03, -6.1427667e-03,\n",
      "        2.6735882e-03, -4.3959129e-03,  3.9032521e-03,  6.0282467e-04,\n",
      "       -5.9801438e-03, -5.0239218e-03, -2.1795244e-03,  8.5901417e-04,\n",
      "        3.3476742e-03,  2.1481616e-03, -2.6519229e-03,  3.3302135e-03,\n",
      "       -3.3548637e-03, -3.4847178e-03, -3.1961182e-03,  7.2376849e-04,\n",
      "        1.9518782e-03,  1.3509091e-03, -7.9227928e-03, -2.5664903e-03,\n",
      "        4.5884692e-04, -5.5935853e-03, -4.8157205e-03,  2.0288816e-03,\n",
      "        2.4438326e-03, -9.1655599e-04, -4.0194630e-03, -5.4637897e-03,\n",
      "       -5.3192978e-03, -3.4000534e-03, -1.6060586e-03,  1.5284423e-03,\n",
      "        8.8017825e-03, -1.6442934e-03, -3.1527192e-03,  6.5926870e-05,\n",
      "        1.6256399e-03, -3.5175139e-03,  1.8200672e-03,  2.1811822e-04,\n",
      "        1.1222010e-03,  7.7779323e-04,  2.1125630e-03, -4.0282924e-03,\n",
      "       -2.3044152e-03, -3.3899664e-04, -5.4023378e-03, -4.6702465e-03,\n",
      "        1.3343969e-03,  3.1323740e-03,  9.0583693e-04, -8.8274600e-03,\n",
      "        2.7001831e-03, -2.5498350e-03,  4.1777059e-04, -2.6973027e-03,\n",
      "       -4.1293693e-03,  8.0700387e-04, -3.0066646e-03,  1.8310297e-05,\n",
      "       -2.2970291e-03,  9.5404766e-04, -1.9620098e-03,  1.6901597e-03,\n",
      "       -5.6661898e-04, -4.5452109e-03,  5.3969570e-03, -1.7377808e-03,\n",
      "        9.8933786e-05, -1.7970110e-03,  9.4340634e-05,  2.9621779e-03,\n",
      "        3.1230943e-03, -1.0631453e-02,  7.4389629e-04,  1.1891123e-03,\n",
      "        4.1289517e-05,  2.0218864e-03,  1.3664851e-03, -4.5630545e-03,\n",
      "        1.0276384e-03,  1.2250122e-03, -1.9530896e-03,  4.5932964e-03,\n",
      "       -1.7688123e-03,  7.9152698e-04, -2.6681598e-03, -8.0584446e-03,\n",
      "       -6.9317906e-03,  6.6196984e-03, -2.2923527e-03,  3.3460755e-04,\n",
      "        2.5797149e-03,  3.2736256e-04, -1.8692722e-03, -9.2382554e-04,\n",
      "       -4.4697630e-03,  6.0768002e-03, -4.0766085e-05, -3.4207439e-03,\n",
      "        4.1444530e-03, -4.2871069e-03,  2.5705397e-03,  6.5127196e-04,\n",
      "       -8.9543435e-05, -2.2069281e-03, -1.9718527e-03,  1.7677813e-03,\n",
      "       -3.9359117e-03, -1.4507518e-03,  2.5936263e-03, -5.1024272e-03,\n",
      "       -1.0349955e-03, -4.2227062e-04,  1.8314162e-03,  6.3751512e-03,\n",
      "       -8.2990306e-04,  5.1425803e-03,  3.7962697e-03,  4.9550859e-03,\n",
      "       -5.3843658e-04,  4.4781109e-03, -1.4426367e-03,  2.4645736e-03,\n",
      "        1.4594668e-03, -1.6249033e-03, -4.7053801e-04,  3.2373122e-04,\n",
      "       -3.8257590e-03,  5.9334314e-03,  4.4727740e-03, -2.4446452e-03,\n",
      "        2.4327706e-03,  3.0504328e-03,  2.3729024e-03,  3.9426382e-03,\n",
      "       -5.5267597e-03,  2.0639219e-03, -1.0422685e-02,  1.9053805e-04,\n",
      "       -1.3269326e-03, -5.9535485e-03,  5.6832805e-03, -7.7954913e-04,\n",
      "       -3.9478471e-03,  4.5627332e-03,  2.0248382e-03,  1.8566291e-04,\n",
      "       -6.2051085e-03,  4.2095319e-03,  1.9666636e-03, -6.1769993e-03,\n",
      "       -3.5370106e-04, -1.7936896e-03, -4.7194064e-03, -7.4151848e-03,\n",
      "        1.3155513e-04,  1.6630838e-03, -1.6211007e-03, -6.6498322e-03,\n",
      "        3.1147571e-03, -1.8462784e-03, -3.3729791e-04, -9.1577426e-04,\n",
      "        8.7528359e-03,  3.9143353e-03, -8.8625780e-04, -6.5604495e-03,\n",
      "        2.7093303e-03,  7.2530634e-03,  1.1548776e-02,  1.6415340e-03,\n",
      "        1.4500708e-03,  4.3354118e-03, -5.1228870e-03, -1.4420989e-03,\n",
      "        2.1772529e-03,  2.5368365e-03,  1.4809577e-03,  4.1056401e-04,\n",
      "        1.0748866e-03,  1.6636349e-04, -2.6377284e-03,  1.5360421e-03],\n",
      "      dtype=float32)]\n",
      "dense_2\n",
      "[array([[-0.03416731,  0.04145459,  0.02062372, ..., -0.00746409,\n",
      "         0.03746865,  0.22612013],\n",
      "       [-0.01569694,  0.08552387,  0.1401457 , ..., -0.12658873,\n",
      "         0.15026644,  0.04099594],\n",
      "       [ 0.05054515,  0.00716379, -0.13515498, ...,  0.00904564,\n",
      "        -0.2660891 ,  0.2033718 ],\n",
      "       ...,\n",
      "       [ 0.2369343 ,  0.04813452,  0.24834165, ..., -0.17977062,\n",
      "         0.20939131,  0.2541819 ],\n",
      "       [ 0.21652828,  0.21023296, -0.10113487, ..., -0.19120917,\n",
      "        -0.03579456, -0.18745387],\n",
      "       [ 0.03722173,  0.00262877,  0.18321092, ...,  0.11051652,\n",
      "         0.16704673, -0.11034822]], dtype=float32), array([ 3.0273672e-03, -5.1218844e-03, -3.1793790e-03, -2.5414100e-03,\n",
      "        9.7446347e-04, -3.2416134e-04,  4.6211237e-05, -1.7831427e-03],\n",
      "      dtype=float32)]\n",
      "lambda_1\n",
      "[]\n",
      "gaussian_noise_1\n",
      "[]\n",
      "dense_3\n",
      "[array([[-0.03932819,  0.05265895,  0.05481428, ...,  0.00316251,\n",
      "         0.05698194, -0.01875618],\n",
      "       [-0.1152171 , -0.26441553, -0.18574552, ...,  0.0588248 ,\n",
      "        -0.16874959, -0.14812393],\n",
      "       [-0.13596372,  0.22127059,  0.08729694, ..., -0.19744541,\n",
      "         0.02944796, -0.1004849 ],\n",
      "       ...,\n",
      "       [-0.02700379, -0.0046939 ,  0.2837818 , ...,  0.02686541,\n",
      "        -0.08611549,  0.20912786],\n",
      "       [-0.13466139,  0.0051733 ,  0.11856303, ..., -0.17642431,\n",
      "         0.04594678,  0.01765984],\n",
      "       [-0.1232348 ,  0.00455001, -0.20426722, ...,  0.10984755,\n",
      "         0.25015447, -0.1926976 ]], dtype=float32), array([ 2.14214213e-02, -8.30851682e-03, -1.24680446e-02,  5.48664387e-03,\n",
      "       -2.31566392e-02,  1.54700009e-02, -5.97318187e-02, -2.27906406e-02,\n",
      "       -7.87459500e-03,  5.88091835e-03,  1.01543833e-02, -1.55329257e-02,\n",
      "        2.63235271e-02,  2.59886533e-02,  2.50620022e-03, -3.09319422e-02,\n",
      "        3.56574915e-02, -2.06912868e-02, -1.32108899e-03, -5.80346794e-04,\n",
      "       -1.45778051e-02,  8.71630199e-03,  3.38572287e-03,  4.58566435e-02,\n",
      "        5.65255061e-03, -6.52499264e-04, -3.86024872e-03, -2.85270400e-02,\n",
      "        1.96722690e-02,  2.40199771e-02, -7.09633343e-03, -1.84592176e-02,\n",
      "        3.37479822e-02,  3.42715643e-02,  2.19357833e-02,  1.30836833e-02,\n",
      "       -3.57348006e-03, -2.12402232e-02, -5.09394752e-03,  2.48946771e-02,\n",
      "       -2.43458040e-02, -1.32493163e-02,  1.15179466e-02,  3.26385088e-02,\n",
      "       -1.28069529e-02,  1.37731768e-02, -1.46074407e-03, -1.54219167e-02,\n",
      "       -2.53013568e-03,  9.69585218e-03, -6.00574166e-03, -1.31635517e-02,\n",
      "        8.25762469e-03,  2.81552151e-02, -9.52166319e-03, -1.43801505e-02,\n",
      "        1.94177218e-02, -1.92188397e-02,  4.97113797e-04, -2.19240282e-02,\n",
      "        5.43272169e-03, -4.13246118e-02, -2.83412971e-02,  6.94637932e-03,\n",
      "        8.82979017e-03, -3.27300164e-03,  1.66896749e-02,  2.51758043e-02,\n",
      "       -3.75698656e-02,  1.92606989e-02, -1.01122819e-02, -5.37607260e-03,\n",
      "        1.71322320e-02, -8.89435131e-03, -2.15128753e-02,  1.11251492e-02,\n",
      "        1.77896768e-02, -4.16933442e-04,  2.22788844e-03,  2.91097946e-02,\n",
      "       -7.83170853e-03, -2.40271632e-03, -4.89294017e-03,  2.99602561e-02,\n",
      "       -1.96045823e-02, -1.43702589e-02, -1.96054485e-03,  2.70471047e-03,\n",
      "       -2.84288544e-02, -1.25791188e-02,  2.71931570e-03, -1.08326888e-02,\n",
      "       -2.03965674e-03,  1.44706657e-02,  2.18480583e-02,  2.23077536e-02,\n",
      "        3.57704870e-02,  1.85440797e-02, -3.13347671e-03,  6.67265942e-03,\n",
      "       -1.30653102e-02,  1.42549472e-02, -2.46245246e-02, -8.68433795e-04,\n",
      "        2.82576471e-03,  1.96931176e-02,  5.24302386e-03,  2.86355652e-02,\n",
      "       -1.15578156e-02,  2.48870924e-02,  1.04834335e-02,  1.02461297e-02,\n",
      "       -3.01788673e-02,  1.79826822e-02, -4.06218767e-02,  3.65197193e-03,\n",
      "       -1.01311726e-03,  8.63065885e-04,  1.25834802e-02, -1.41357593e-02,\n",
      "       -1.15510346e-02, -4.83431686e-05,  3.81216058e-03, -3.49591151e-02,\n",
      "       -2.35677734e-02,  1.64954290e-02,  1.01243835e-02, -1.06910411e-02,\n",
      "        2.51418948e-02, -2.39095148e-02, -1.45627549e-02, -2.97932103e-02,\n",
      "        3.40768434e-02,  4.54932489e-02,  2.95263585e-02, -1.69091225e-02,\n",
      "       -2.02099234e-02, -1.98646262e-02,  2.52504330e-02, -1.42478496e-02,\n",
      "       -1.76166650e-02,  1.65022332e-02,  3.32384929e-03,  2.42253914e-02,\n",
      "       -1.15229292e-02, -1.34718465e-02,  1.25561384e-02,  3.02820969e-02,\n",
      "       -7.57367164e-03, -3.92021006e-03,  1.74110904e-02, -1.79294981e-02,\n",
      "        6.83509745e-03, -3.89234186e-03, -6.51360536e-03, -5.33290543e-02,\n",
      "        1.90110924e-03, -3.55295949e-02, -1.47291198e-02,  8.05301312e-03,\n",
      "        1.65859051e-02,  1.08365137e-02, -2.01084018e-02, -1.99199119e-03,\n",
      "        4.26588878e-02,  5.09450026e-03,  1.28465705e-03,  7.03806849e-03,\n",
      "        1.74199697e-02,  5.00109978e-02,  3.76542145e-03, -3.43191973e-03,\n",
      "       -2.85037756e-02, -1.10151982e-02,  2.07010154e-02,  7.39732804e-03,\n",
      "       -7.72622786e-03,  3.04758959e-02,  1.70587283e-02, -6.25861716e-03,\n",
      "       -1.64099745e-02,  1.62327103e-02,  1.63463429e-02,  4.74664476e-03,\n",
      "       -2.42916518e-03,  2.22722739e-02,  8.10382981e-03, -3.11099533e-02,\n",
      "        2.05160887e-03,  3.25969905e-02,  1.41391745e-02,  2.96989363e-02,\n",
      "        5.81496581e-03, -7.74353929e-03,  9.30880196e-03,  1.20769173e-03,\n",
      "       -2.11414434e-02, -4.04746365e-03,  1.73172876e-02, -2.33503412e-02,\n",
      "        2.13550087e-02, -1.93561763e-02, -6.44520763e-03, -1.12157334e-02,\n",
      "       -6.75220974e-03, -3.04013696e-02, -1.94528606e-03,  1.16702337e-02,\n",
      "        3.96289770e-03, -2.75685056e-03,  2.94402824e-03,  1.19166411e-02,\n",
      "       -1.46732088e-02,  2.46599014e-03,  3.10455402e-03,  2.37075314e-02,\n",
      "       -2.70441547e-02,  2.66022258e-03,  4.90347203e-03, -9.68363043e-03,\n",
      "       -1.80308521e-02, -3.69150452e-02,  6.14273874e-03,  4.96635912e-03,\n",
      "       -1.61839146e-02, -6.06379099e-03, -6.00672280e-03,  1.25756739e-02,\n",
      "       -1.68488882e-02,  4.53400565e-03,  1.10442834e-02,  2.47837398e-02,\n",
      "        2.05176547e-02, -2.82764924e-03,  9.55350604e-03,  1.41835096e-03,\n",
      "       -7.50818010e-03,  2.80707283e-03, -1.52660599e-02, -4.86136088e-03,\n",
      "        2.87569836e-02,  8.99813976e-03,  3.93580012e-02, -1.23160388e-02,\n",
      "        4.90322486e-02,  6.26044208e-03,  5.25359772e-02, -1.10995891e-02,\n",
      "        3.80673073e-02, -1.16766719e-02,  3.42242755e-02,  2.55901534e-02,\n",
      "        6.76558632e-03, -4.68597375e-03,  2.04208605e-02,  8.36411119e-03],\n",
      "      dtype=float32)]\n",
      "dense_4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[-0.1363531 ,  0.13229367, -0.07839827, ..., -0.01097638,\n",
      "         0.15951377, -0.07576041],\n",
      "       [ 0.06030845,  0.01827411,  0.02529485, ..., -0.02777753,\n",
      "        -0.1418681 ,  0.05839665],\n",
      "       [ 0.0020199 ,  0.00243485,  0.17763181, ...,  0.13269949,\n",
      "        -0.04966784, -0.02599123],\n",
      "       ...,\n",
      "       [-0.03178185, -0.0424644 , -0.19944906, ..., -0.0686636 ,\n",
      "         0.1331853 , -0.0902095 ],\n",
      "       [ 0.09154726, -0.00455543,  0.05101276, ..., -0.13848518,\n",
      "        -0.1880211 ,  0.12054257],\n",
      "       [ 0.06484296, -0.1765913 ,  0.07128783, ..., -0.04641288,\n",
      "        -0.04211171, -0.0888099 ]], dtype=float32), array([-4.18724827e-02, -1.47551205e-02,  5.93607798e-02, -3.79062742e-02,\n",
      "       -6.01266213e-02, -2.88586691e-02, -9.05904267e-03,  3.90001386e-02,\n",
      "       -6.86919456e-03,  3.35555375e-02, -5.12454547e-02, -4.10143882e-02,\n",
      "       -1.61296502e-02, -6.14574999e-02,  1.50016753e-03,  4.14353870e-02,\n",
      "       -7.32615292e-02, -1.22748323e-01, -1.13528315e-02,  2.33945902e-03,\n",
      "       -6.98594330e-03,  7.83220083e-02, -3.07359565e-02,  5.29986471e-02,\n",
      "        6.98121935e-02,  2.49430165e-02, -8.95847529e-02, -2.14647800e-02,\n",
      "       -1.23461455e-01,  1.20046651e-02, -4.45276089e-02,  8.31681676e-03,\n",
      "        2.32495666e-02,  4.27337177e-03, -1.57476854e-04, -2.51479913e-02,\n",
      "       -5.29494733e-02,  4.51211669e-02,  8.79625529e-02,  7.70380571e-02,\n",
      "       -7.91247115e-02, -7.33171627e-02, -5.18676043e-02, -3.30685377e-02,\n",
      "       -4.26567830e-02, -1.93527862e-02, -7.14456942e-03, -6.01459714e-03,\n",
      "       -7.48849958e-02, -3.28497514e-02, -3.87974232e-02,  3.42040346e-03,\n",
      "        3.32991667e-02,  1.48930205e-02, -6.76112920e-02, -5.45999296e-02,\n",
      "       -5.14234640e-02,  1.40564395e-02, -7.92490914e-02, -2.37771273e-02,\n",
      "       -1.56610068e-02,  2.08559982e-03, -3.02781072e-02,  8.78395811e-02,\n",
      "        4.59791087e-02, -7.93909421e-04, -3.20096277e-02,  1.58042330e-02,\n",
      "        2.97747855e-03,  3.62151936e-02,  1.93998404e-02,  4.27383231e-03,\n",
      "       -2.59328540e-02,  3.11625116e-02, -2.06329785e-02,  7.79094547e-03,\n",
      "        1.77692883e-02,  1.79556981e-02,  3.08493413e-02,  1.80016793e-02,\n",
      "       -1.91668198e-02, -3.20829675e-02,  1.79002322e-02, -1.03544805e-03,\n",
      "        1.86525434e-02, -1.46878269e-02,  5.53793311e-02,  6.43234625e-02,\n",
      "       -8.33385345e-03,  1.12855360e-02, -1.01312704e-01,  3.24643124e-03,\n",
      "       -6.64228499e-02, -1.69838086e-01,  2.63599101e-02, -2.77441386e-02,\n",
      "       -3.13271135e-02,  2.72782687e-02, -6.89652041e-02, -7.76397735e-02,\n",
      "       -6.05793893e-02, -7.95422718e-02, -1.01785906e-01,  1.35933002e-02,\n",
      "        5.31110838e-02,  2.89299563e-02,  2.03641951e-02,  2.19034664e-02,\n",
      "       -5.62418625e-02, -8.78556445e-03, -2.11856626e-02, -4.39571962e-02,\n",
      "        3.71350069e-03, -4.06304970e-02,  1.32711465e-02, -6.39163405e-02,\n",
      "       -9.01967064e-02, -2.02611610e-02,  5.82392327e-02, -4.07669023e-02,\n",
      "       -3.02749071e-02, -3.49296443e-02,  1.22755498e-01, -6.53774366e-02,\n",
      "       -2.00146101e-02, -4.30235453e-02, -6.31301552e-02,  8.22078343e-03,\n",
      "        1.57009408e-01, -7.07217976e-02, -7.25439265e-02, -1.93241835e-02,\n",
      "       -6.00695051e-02, -7.74225518e-02,  5.52956276e-02,  1.87148638e-02,\n",
      "       -3.71571966e-02, -4.48718145e-02, -6.35464936e-02, -4.88025434e-02,\n",
      "        2.30576340e-02, -2.67713107e-02,  1.96424518e-02,  3.36363800e-02,\n",
      "       -1.43484809e-02, -9.43761170e-02,  6.92553148e-02, -8.46639201e-02,\n",
      "       -5.06073050e-02,  3.71283144e-02,  6.98532769e-03, -6.00926578e-02,\n",
      "       -3.06575056e-02, -4.95424122e-03, -7.13519529e-02, -3.79150994e-02,\n",
      "        5.36742881e-02, -6.41108602e-02,  8.06309432e-02,  2.74996483e-03,\n",
      "        3.54999155e-02, -8.01014528e-02,  2.48594005e-02,  1.65294844e-03,\n",
      "        3.29295583e-02,  5.03797196e-02,  6.32689670e-02,  8.51620454e-04,\n",
      "       -7.02023506e-03, -2.69317571e-02,  5.22709079e-02, -4.03460078e-02,\n",
      "       -3.81654361e-03, -3.38351466e-02,  5.17634898e-02, -5.88560775e-02,\n",
      "       -1.71530358e-02, -3.44129615e-02,  1.10008284e-01,  2.82663126e-02,\n",
      "        3.10085565e-02, -2.35495828e-02, -1.69063583e-02, -5.18516563e-02,\n",
      "        4.28928435e-02, -8.44049826e-03,  1.09536322e-02, -1.77845731e-02,\n",
      "        1.79010183e-02,  2.87606921e-02,  4.95008640e-02, -8.09326544e-02,\n",
      "       -2.65766721e-04, -6.78728446e-02, -3.34720351e-02, -4.57086824e-02,\n",
      "        2.71271002e-02,  2.11424883e-02, -1.00134499e-02, -1.13846676e-03,\n",
      "       -6.64415359e-02, -8.10181126e-02, -4.19018306e-02, -5.87055571e-02,\n",
      "       -2.55749896e-02,  5.64515106e-02,  4.97891121e-02,  1.91277917e-02,\n",
      "       -8.75953808e-02, -2.41281595e-02,  8.95562097e-02,  6.05167449e-02,\n",
      "       -5.19173704e-02,  1.07117193e-02, -2.14598402e-02,  6.55174702e-02,\n",
      "       -4.06405590e-02, -4.70726267e-02,  2.22951025e-02, -3.69664766e-02,\n",
      "        4.96163517e-02,  4.08285186e-02, -3.76156345e-02, -4.61222678e-02,\n",
      "       -3.58542800e-02, -7.55914748e-02, -2.99753807e-03,  3.31271365e-02,\n",
      "       -9.86008495e-02,  1.00141913e-01,  4.50866967e-02,  1.64851900e-02,\n",
      "        1.29380655e-02, -5.59375100e-02,  1.87434889e-02,  3.38333077e-03,\n",
      "       -3.83826382e-02, -5.67197725e-02,  3.79757881e-02, -4.13683243e-02,\n",
      "       -1.30741447e-01,  6.32032286e-03, -2.07190514e-02,  3.19215618e-02,\n",
      "        9.46891308e-03,  1.70590058e-02,  3.33606265e-02, -1.06259286e-01,\n",
      "       -2.57300418e-02, -2.25827303e-02,  5.11600561e-02, -5.06593846e-02,\n",
      "       -1.95854809e-03, -9.78420256e-04, -1.19157173e-02, -4.98482883e-02],\n",
      "      dtype=float32)]\n",
      "dict_keys(['loss'])\n"
     ]
    }
   ],
   "source": [
    "# # print the layers and weights\n",
    "# for lay in autoencoder_SR.layers:\n",
    "#     print(lay.name)\n",
    "#     print(lay.get_weights())\n",
    "# print(hist.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making encoder from full autoencoder\n",
    "encoder_SR = Model(input_signal, encoded2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making decoder from full autoencoder\n",
    "encoded_input = Input(shape=(n_channel,))\n",
    "\n",
    "# 3: decoder should also be modified\n",
    "# deco = autoencoder.layers[-3](encoded_input)\n",
    "deco = autoencoder_SR.layers[-2](encoded_input)\n",
    "deco_1 = autoencoder_SR.layers[-1](deco)\n",
    "decoder_SR = Model(encoded_input, deco_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving autoencoder_SR\n",
    "# HDF5 file, you have to pip3 install h5py if don't have it\n",
    "# if you want to save model then remove below comment\n",
    "# import h5py\n",
    "# autoencoder_SR.save('autoencoder_SR.h5')\n",
    "# encoder_SR.save('encoder_SR.h5')\n",
    "# decoder_SR.save('decoder_SR.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the S-R link BLER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating data for checking BLER\n",
    "N = 1*10**5\n",
    "test_label = np.random.randint(M,size=N)\n",
    "test_data = []\n",
    "\n",
    "for i in test_label:\n",
    "    temp = np.zeros(M)\n",
    "    temp[i] = 1\n",
    "    test_data.append(temp)\n",
    "    \n",
    "test_data = np.array(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNR: -2 BLER: 0.64052\n",
      "SNR: 0 BLER: 0.43181\n",
      "SNR: 2 BLER: 0.21372\n",
      "SNR: 4 BLER: 0.06498\n",
      "SNR: 6 BLER: 0.01087\n",
      "SNR: 8 BLER: 0.00089\n"
     ]
    }
   ],
   "source": [
    "# calculating BLER\n",
    "EbNodB_range = list(frange(-2,8.5,2))\n",
    "# EbNodB_range = list(frange(8,8.5,1))\n",
    "bler = [None]*len(EbNodB_range)\n",
    "for n in range(0,len(EbNodB_range)):\n",
    "    EbNo=10.0**(EbNodB_range[n]/10.0)\n",
    "    noise_std = np.sqrt(1/(2*R*EbNo))\n",
    "    noise_mean = 0\n",
    "    no_errors = 0\n",
    "    nn = N\n",
    "    noise = noise_std * np.random.randn(nn,n_channel)\n",
    "    encoded_signal = encoder_SR.predict(test_data) \n",
    "    final_signal = encoded_signal + noise\n",
    "    pred_final_signal =  decoder_SR.predict(final_signal)\n",
    "    pred_output = np.argmax(pred_final_signal,axis=1)\n",
    "    no_errors = (pred_output != test_label)\n",
    "    no_errors =  no_errors.astype(int).sum()\n",
    "    bler[n] = no_errors / nn \n",
    "    print ('SNR:',EbNodB_range[n],'BLER:',bler[n])\n",
    "    # use below line for generating matlab like matrix which can be copy and paste for plotting ber graph in matlab\n",
    "    # print(bler[n], \" \",end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ploting ber curve\n",
    "# save(ber, 'BLER_aotuencoder')\n",
    "np.savetxt('BLER-SRlink-AE-ROB.txt', bler)\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import interpolate\n",
    "plt.plot(EbNodB_range, bler, 'bo',label='Autoencoder-SRlink')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('SNR (dB)')\n",
    "plt.ylabel('BLER for S-R link')\n",
    "plt.grid()\n",
    "plt.legend(loc='upper right',ncol = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
